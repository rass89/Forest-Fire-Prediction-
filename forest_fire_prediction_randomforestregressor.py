# -*- coding: utf-8 -*-
"""Forest Fire Prediction RandomForestRegressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIBRv-qX4NS-3uovtbN4p0RVSbz6arvu
"""

# Commented out IPython magic to ensure Python compatibility.
import datetime as dt

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestRegressor

forest = pd.read_csv('/content/fire_archive.csv')

forest.head()

forest.shape

forest.isnull().sum()

forest.describe()

plt.figure(figsize=(10, 10))
sns.heatmap(forest.corr(),annot=True,cmap='viridis',linewidths=.5)

# Convert 'acq_date' to datetime objects if it's not already
forest['acq_date'] = pd.to_datetime(forest['acq_date'])

# Extract numeric features from the date
forest['acq_year'] = forest['acq_date'].dt.year
forest['acq_month'] = forest['acq_date'].dt.month
forest['acq_day'] = forest['acq_date'].dt.day

# Drop the original 'acq_date' column and other non-numeric columns
# You might need to identify other non-numeric columns and include them here
numeric_forest = forest.select_dtypes(include=np.number)


plt.figure(figsize=(10, 10))
sns.heatmap(numeric_forest.corr(), annot=True, cmap='viridis', linewidths=.5)

forest = forest.drop(['track'], axis = 1)

# By the way from the the dataset we are not finding if the forest fire happens or not, we are trying to find the confidence of the forest
# fire happening. They may seem to be the same thing but there is a very small difference between them, try to find that :)

# Finding categorical data
print("The scan column")
print(forest['scan'].value_counts())
print()
print("The aqc_time column")
print(forest['acq_time'].value_counts())
print()
print("The satellite column")
print(forest['satellite'].value_counts())
print()
print("The instrument column")
print(forest['instrument'].value_counts())
print()
print("The version column")
print(forest['version'].value_counts())
print()
print("The daynight column")
print(forest['daynight'].value_counts())
print()

# From the above data we can see that some columns have just one value reccurring in them, meaning they are not valuable to us
# So we will drop them altogether.
# Thus only satellite and daynight column are the only categorical type.

# But we can even use the scan column to restructure it into a categorical data type column. Which we will be doing in just a while.

forest = forest.drop(['instrument', 'version'], axis = 1)

forest.head()

daynight_map = {"D": 1, "N": 0}
satellite_map = {"Terra": 1, "Aqua": 0}

forest['daynight'] = forest['daynight'].map(daynight_map)
forest['satellite'] = forest['satellite'].map(satellite_map)

forest.head()

# Looking at another columns type
forest['type'].value_counts()

types = pd.get_dummies(forest['type'])
forest = pd.concat([forest, types], axis=1)

forest = forest.drop(['type'], axis = 1)
forest.head()

# Renaming columns for better understanding

forest = forest.rename(columns={0: 'type_0', 2: 'type_2', 3: 'type_3'})

# Now I mentioned we will be converting scan column to categorical type, we will be doing this using binning method.
# Range for this columns was 1 to 4.8

bins = [0, 1, 2, 3, 4, 5]
labels = [1,2,3,4,5]
forest['scan_binned'] = pd.cut(forest['scan'], bins=bins, labels=labels)

forest.head()

# Converting the datatype to datetype from string or numpy.

forest['acq_date'] = pd.to_datetime(forest['acq_date'])

# Now we will be dropping scan column and handle date type data - we can extract useful information from these datatypes
# just like we do with categorical data.

forest = forest.drop(['scan'], axis = 1)

forest['year'] = forest['acq_date'].dt.year

forest.head()

forest['month'] = forest['acq_date'].dt.month
forest['day'] = forest['acq_date'].dt.day

forest.shape

# Separating our target varibale:

y = forest['confidence']
fin = forest.drop(['confidence', 'acq_date', 'acq_time', 'bright_t31', 'type_0'], axis = 1)

plt.figure(figsize=(10, 10))
sns.heatmap(fin.corr(),annot=True,cmap='viridis',linewidths=.5)

fin.head()

Xtrain, Xtest, ytrain, ytest = train_test_split(fin.iloc[:, :500], y, test_size=0.2)

random_model = RandomForestRegressor(n_estimators=300, random_state = 42, n_jobs = -1)

#Fit
random_model.fit(Xtrain, ytrain)

y_pred = random_model.predict(Xtest)

#Checking the accuracy
random_model_accuracy = round(random_model.score(Xtrain, ytrain)*100,2)
print(round(random_model_accuracy, 2), '%')

#Checking the accuracy
random_model_accuracy1 = round(random_model.score(Xtest, ytest)*100,2)
print(round(random_model_accuracy1, 2), '%')

# Save the trained model as a pickle string. /content/fire_archive.csv
import pickle

saved_model = pickle.dump(random_model, open('/content/ForestModelOld.pickle','wb'))

# The accuracy is not so great, plus the model is overfitting
# So we use RandomCV

random_model.get_params()

"""
n_estimators = number of trees in the foreset
max_features = max number of features considered for splitting a node
max_depth = max number of levels in each decision tree
min_samples_split = min number of data points placed in a node before the node is split
min_samples_leaf = min number of data points allowed in a leaf node
bootstrap = method for sampling data points (with or without replacement)
"""

from sklearn.model_selection import RandomizedSearchCV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 300, stop = 500, num = 20)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(15, 35, num = 7)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 3, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
                }
print(random_grid)

# Random search of parameters, using 3 fold cross validation,
# search across 100 different combinations, and use all available cores

# n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation
rf_random = RandomizedSearchCV(estimator = random_model, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42)
# Fit the random search model
rf_random.fit(Xtrain, ytrain)

rf_random.best_params_

random_new = RandomForestRegressor(n_estimators = 394, min_samples_split = 2, min_samples_leaf = 1, max_features = 'sqrt',
                                      max_depth = 25, bootstrap = True)

#Fit
random_new.fit(Xtrain, ytrain)

y_pred1 = random_new.predict(Xtest)

#Checking the accuracy
random_model_accuracy1 = round(random_new.score(Xtrain, ytrain)*100,2)
print(round(random_model_accuracy1, 2), '%')

#Checking the accuracy
random_model_accuracy2 = round(random_new.score(Xtest, ytest)*100,2)
print(round(random_model_accuracy2, 2), '%')

# Save the trained model as a pickle string.
saved_model = pickle.dump(random_new, open('/content/ForestModel.pickle','wb'))

# Load the pickled model - reg_from_pickle = pickle.load(saved_model)
# Use the loaded pickled model to make predictions - reg_from_pickle.predict(X_test)

pip install bz2file

# So I installed bz2file, which is used to compress data. This is a life saving package fpor those who have low spaces on
# their disk but want to store or use large datasets. Now the pickled file was over 700 mb in size which when used bz2
# compressed in into a file of size 93 mb.

import bz2

compressionLevel = 9
source_file = '/content/ForestModel.pickle' # this file can be in a different format, like .csv or others...
destination_file = '/content/ForestModel.bz2'

with open(source_file, 'rb') as data:
    tarbz2contents = bz2.compress(data.read(), compressionLevel)

fh = open(destination_file, "wb")
fh.write(tarbz2contents)
fh.close()

